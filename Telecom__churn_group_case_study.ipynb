{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a1be55",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study(Machine learning -II)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9034fb",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "### Business problem overview\n",
    "\n",
    ". In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    ". For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    ". To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    ". In this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e2f82",
   "metadata": {},
   "source": [
    "### Definitions of churn\n",
    ". There are various ways to define churn, such as:\n",
    "\n",
    "### Revenue-based churn:\n",
    ". Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\n",
    "\n",
    "The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n",
    "\n",
    "### Usage-based churn:\n",
    "Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n",
    "\n",
    "A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n",
    "\n",
    "In this project, we will use the usage-based definition to define churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687362a1",
   "metadata": {},
   "source": [
    "# Objective\n",
    ". To Predict the customers who are about to churn from a telecom operator\n",
    ". Business Objective is to predict the High Value Customers only\n",
    ". We need to predict Churn on the basis of Action Period (Churn period data needs to be deleted after labelling)\n",
    "  Churn would be based on Usage\n",
    "\n",
    "### Requirement:\n",
    "\n",
    ". Churn Prediction Model\n",
    ". Best Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce880e4",
   "metadata": {},
   "source": [
    "# Steps to Approach The  Best Solution For This Case Study\n",
    "There are mainly 6 steps\n",
    "#### Step 1 :\n",
    ". Data reading\n",
    ". Data Understanding\n",
    ". Data Cleaning\n",
    "     Imputing missing values \n",
    "#### Step-2 :\n",
    "Need to Filter high value customers\n",
    "\n",
    "#### Step-3 :\n",
    "Derive churn\n",
    "   need to Derive the Target Variable\n",
    "   \n",
    "#### Step-4 :\n",
    "Data Preparation\n",
    "  .Derived variable\n",
    "  .EDA\n",
    "  .Split data in to train and test sets\n",
    "  .Performing Scaling\n",
    " \n",
    "#### Step-5 :\n",
    ". Handle class imbalance\n",
    ". Dimensionality Reduction using PCA\n",
    ".Classification models to predict Churn (Use various Models )\n",
    "\n",
    "#### Step-6 :\n",
    ".Model Evaluation\n",
    ".Prepare Model for Predictor variables selection (Prepare multiple models & choose the best one)\n",
    "\n",
    "Finally we need to give best Summarize to the company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689c53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb58e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd4087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126f56a",
   "metadata": {},
   "source": [
    "## Import  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8069c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the logistic regression module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing 'variance_inflation_factor' or VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Import RFE for RFE selection\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Importing statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Importing the precision recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Importing evaluation metrics from scikitlearn \n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# To suppress the warnings which will be raised\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Displaying all Columns without restrictions\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "# import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.metrics import sensitivity_specificity_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f644e",
   "metadata": {},
   "source": [
    "# Step 1 : \n",
    "\n",
    "# i.Data reading \n",
    "\n",
    "# ii.Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21836f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "churn = pd.read_csv(r\"C:\\Users\\Indraneel Dasari\\Desktop\\telecom_churn_data (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at initial rows of the data\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d641f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create backup of data\n",
    "original = churn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the last 5 rows\n",
    "churn.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns of data\n",
    "churn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the numerical columns data distribution statistics\n",
    "churn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dataframe for null and datatype \n",
    "churn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature type summary\n",
    "churn.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "churn.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af320795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null value percentage\n",
    "churn.isna().sum()/churn.isna().count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f489f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for shape of a data set\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b01a71",
   "metadata": {},
   "source": [
    "This telecom dataset has 99999 rows and 226 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb701a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the duplicates\n",
    "churn.drop_duplicates(subset=None, inplace=True)\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8df6d",
   "metadata": {},
   "source": [
    "There are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the size of data\n",
    "churn.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the axes of data\n",
    "churn.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dimensions of data\n",
    "churn.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47748f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the values of data\n",
    "churn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns\n",
    "pd.DataFrame(churn.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7892c",
   "metadata": {},
   "source": [
    "# iii. Data Cleaning , Imputing missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at missing value ratio in each column\n",
    "churn.isnull().sum()*100/churn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e60677",
   "metadata": {},
   "source": [
    "As We can see more then **74%** values for **recharge** related data are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb333c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some recharge columns have minimum value of 1 while some don't have\n",
    "recharge_cols = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "                 'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_2g_9',\n",
    "                 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8', 'count_rech_3g_9',\n",
    "                 'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9',\n",
    "                 'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "                 ]\n",
    "\n",
    "churn[recharge_cols].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7da23",
   "metadata": {},
   "source": [
    " We can create new feature as total_rech_amt_data using total_rech_data and av_rech_amt_data to capture amount utilized by customer for data.\n",
    "\n",
    " Also as the minimum value is 1 we can impute the NA values by 0, Considering there were no recharges done by the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also observed that the recharge date and the recharge value are missing together which means the customer didn't recharge\n",
    "churn.loc[churn.total_rech_data_6.isnull() & churn.date_of_last_rech_data_6.isnull(), [\"total_rech_data_6\", \"date_of_last_rech_data_6\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c1f91",
   "metadata": {},
   "source": [
    "In the recharge variables where minumum value is 1, we can impute missing values with zeroes since it means customer didn't recharge their numbere that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b204f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of recharge columns where we will impute missing values with zeroes\n",
    "zero_impute = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "        'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "        'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with 0\n",
    "churn[zero_impute] = churn[zero_impute].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to  make sure the values are imputed correctly for that we can check \"Missing value ratio\"\n",
    "churn[zero_impute].isnull().sum()*100/churn.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69733612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can check the \"statistics Summary\"\n",
    "churn[zero_impute].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ae2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create some column name list by there types using description of columns\n",
    "id_cols = ['mobile_number', 'circle_id']\n",
    "\n",
    "date_cols = ['last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9'\n",
    "            ]\n",
    "\n",
    "cat_cols =  ['night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9',\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9'\n",
    "            ]\n",
    "\n",
    "num_cols = [column for column in churn.columns if column not in id_cols + date_cols + cat_cols]\n",
    "\n",
    "# print the number of columns in each list\n",
    "print(\"#ID cols: %d\\n#Date cols:%d\\n#Numeric cols:%d\\n#Category cols:%d\" % (len(id_cols), len(date_cols), len(num_cols), len(cat_cols)))\n",
    "\n",
    "# check if we have missed any column or not\n",
    "print(len(id_cols) + len(date_cols) + len(num_cols) + len(cat_cols) == churn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a410e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id and date columns\n",
    "churn = churn.drop(id_cols + date_cols, axis=1)\n",
    "#check the shape again\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956493de",
   "metadata": {},
   "source": [
    "We will replace missing values in the categorical values with '-1' where '-1' will be a new category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c31989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with '-1' in categorical columns\n",
    "churn[cat_cols] = churn[cat_cols].apply(lambda x: x.fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value ratio\n",
    "churn[cat_cols].isnull().sum()*100/churn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dbe83",
   "metadata": {},
   "source": [
    "Droping variables with more than 70% of missing values (we can call it as threshold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf12f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cols = churn.shape[1]\n",
    "\n",
    "MISSING_THRESHOLD = 0.7\n",
    "\n",
    "include_cols = list(churn.apply(lambda column: True if column.isnull().sum()/churn.shape[0] < MISSING_THRESHOLD else False))\n",
    "\n",
    "drop_missing = pd.DataFrame({'features':churn.columns , 'include': include_cols})\n",
    "drop_missing.loc[drop_missing.include == True,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can drop  some more columns\n",
    "churn = churn.loc[:, include_cols]\n",
    "\n",
    "dropped_cols = churn.shape[1] - initial_cols\n",
    "dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rechecking the shape of a dataframe\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechecking the missing values for how many missing values has left\n",
    "churn.isnull().sum()*100/churn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6105270",
   "metadata": {},
   "source": [
    "only numerical variables have the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [column for column in churn.columns if column not in id_cols + date_cols + cat_cols]\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31399e4f",
   "metadata": {},
   "source": [
    "for remaining numerical variables missing values hear we are imputing with meadian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46aa3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with meadian for num_cols\n",
    "churn[num_cols] = churn[num_cols].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eab4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking for the missing values\n",
    "churn.isnull().sum()*100/churn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b280a",
   "metadata": {},
   "source": [
    "In churn prediction, we assume that there are three phases of customer lifecycle :\n",
    "\n",
    ". The ‘good’ phase [Month 6 & 7]\n",
    ". The ‘action’ phase [Month 8]\n",
    ". The ‘churn’ phase [Month 9]\n",
    "In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348c364",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "\n",
    "# Filter high-value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf4283",
   "metadata": {},
   "source": [
    "Hear we can take good phase ( it means month 6 and 7) data to get high value customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total data recharge amount for June and July --> number of recharges * average recharge amount\n",
    "churn['total_data_rech_6'] = churn.total_rech_data_6 * churn.av_rech_amt_data_6\n",
    "churn['total_data_rech_7'] = churn.total_rech_data_7 * churn.av_rech_amt_data_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a544f",
   "metadata": {},
   "source": [
    "add total data recharge and total recharge to get total combined recharge amount for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd508c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total recharge amount for June and July --> call recharge amount + data recharge amount\n",
    "churn['amt_data_6'] = churn.total_rech_amt_6 + churn.total_data_rech_6\n",
    "churn['amt_data_7'] = churn.total_rech_amt_7 + churn.total_data_rech_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab14850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average recharge done by customer in June and July\n",
    "churn['av_amt_data_6_7'] = (churn.amt_data_6 + churn.amt_data_7)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the 70th percentile recharge amount\n",
    "print(\"Recharge amount at 70th percentile: {0}\".format(churn.av_amt_data_6_7.quantile(0.7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08139642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only those customers who have recharged their mobiles with more than or equal to 70th percentile amount\n",
    "churn_filtered = churn.loc[churn.av_amt_data_6_7 >= churn.av_amt_data_6_7.quantile(0.7), :]\n",
    "churn_filtered = churn_filtered.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete variables created to filter high-value customers\n",
    "churn_filtered = churn_filtered.drop(['total_data_rech_6', 'total_data_rech_7',\n",
    "                                      'amt_data_6', 'amt_data_7', 'av_amt_data_6_7'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11a2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff8d61",
   "metadata": {},
   "source": [
    " hear we're left with 30,001 rows  and 196 columns after selecting the customers who have provided recharge value of more than or equal to the recharge value of the 70th percentile customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52217c",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "\n",
    "# Derive churn\n",
    "\n",
    "Derive churn means hear we are using 9 month(The ‘churn’ phase) data , To get the target variable(In this case stydy they did not provide any target variable we have to derive it from churn phase data)\n",
    "For that, we need to find the derive churn variable using total_ic_mou_9,total_og_mou_9,vol_2g_mb_9 and vol_3g_mb_9 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns to define churn variable (i.e. TARGET Variable)\n",
    "churn_col=['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']\n",
    "churn_filtered[churn_col].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c951a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the churn variable.\n",
    "churn_filtered['churn']=0\n",
    "\n",
    "# Imputing the churn values based on the condition\n",
    "churn_filtered['churn'] = np.where(churn_filtered[churn_col].sum(axis=1) == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the top 10 data\n",
    "churn_filtered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1131895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find out churn/non churn percentage\n",
    "print((churn_filtered['churn'].value_counts()/len(churn))*100)\n",
    "((churn_filtered['churn'].value_counts()/len(churn))*100).plot(kind=\"pie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342733e0",
   "metadata": {},
   "source": [
    "#### ***As we can see that 97% of the customers do not churn, there is a possibility of class imbalance*** \n",
    "Since this variable churn is the target variable, all the columns relating to this variable(i.e. all columns with suffix _9) can be dropped forn the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all the churn phase columns in order to drop then\n",
    "\n",
    "churn_phase_cols = [col for col in churn_filtered.columns if '_9' in col]\n",
    "print(\"The columns from churn phase are:\\n\",churn_phase_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a343e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the selected churn phase columns\n",
    "churn_filtered.drop(churn_phase_cols, axis=1, inplace=True)\n",
    "\n",
    "# The curent dimension of the dataset after dropping the churn related columns\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffc7f7",
   "metadata": {},
   "source": [
    "We can still clean the data by few possible columns relating to the good phase.\n",
    "\n",
    "As we derived few columns in the good phase earlier, we can drop those related columns during creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#churn['total_rech_amt_data_6']=churn['av_rech_amt_data_6'] * churn['total_rech_data_6']\n",
    "# churn['total_rech_amt_data_7']=churn['av_rech_amt_data_7'] * churn['total_rech_data_7']\n",
    "\n",
    "# # Calculating the overall recharge amount for the months 6,7,8 and 9\n",
    "\n",
    "# churn['overall_rech_amt_6'] = churn['total_rech_amt_data_6'] + churn['total_rech_amt_6']\n",
    "# churn['overall_rech_amt_7'] = churn['total_rech_amt_data_7'] + churn['total_rech_amt_7']\n",
    "\n",
    "churn_filtered.drop(['av_rech_amt_data_6',\n",
    "                   'total_rech_data_6','total_rech_amt_6',\n",
    "                  'av_rech_amt_data_7',\n",
    "                   'total_rech_data_7','total_rech_amt_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c9b75",
   "metadata": {},
   "source": [
    "We can also create new columns for the defining the good phase variables and drop the seperate 6th and 7 month variables.\n",
    "\n",
    "Before proceding to check the remaining missing value handling, let us check the collineartity of the indepedent variables and try to understand their dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of column names for each month\n",
    "mon_6_cols = [col for col in churn_filtered.columns if '_6' in col]\n",
    "mon_7_cols = [col for col in churn_filtered.columns if '_7' in col]\n",
    "mon_8_cols = [col for col in churn_filtered.columns if '_8' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the correlation amongst the independent variables, drop the highly correlated ones\n",
    "churn_corr = churn_filtered.corr()\n",
    "churn_corr.loc[:,:] = np.tril(churn_corr, k=-1)\n",
    "churn_corr = churn_corr.stack()\n",
    "churn_corr\n",
    "churn_corr[(churn_corr > 0.80) | (churn_corr < -0.80)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop=['fb_user_6','fb_user_7','fb_user_8','total_rech_amt_8','isd_og_mou_8',\n",
    "             'sachet_2g_8','isd_og_mou_8','total_ic_mou_6','total_ic_mou_8','total_ic_mou_7',               \n",
    "               'std_og_t2t_mou_8','std_og_t2t_mou_7','total_og_mou_8','std_og_t2m_mou_8' ,'total_og_mou_7',\n",
    "             'std_ic_mou_8','std_og_t2t_mou_6' ,'std_og_t2m_mou_7','std_ic_mou_7',]\n",
    "\n",
    "# These columns can be dropped as they are highly collinered with other predictor variables.\n",
    "# criteria set is for collinearity of 85%\n",
    "\n",
    "#  dropping these column\n",
    "churn_filtered.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curent dimension of the dataset after dropping few unwanted columns\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d2ed1",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69275e",
   "metadata": {},
   "source": [
    "# i.Deriving new variables to understand the data \n",
    "\n",
    "# ii.EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefde9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a column called 'aon'\n",
    "\n",
    "# we can derive new variables from this to explain the data w.r.t churn.\n",
    "\n",
    "# creating a new variable 'tenure'\n",
    "churn_filtered['tenure'] = (churn_filtered['aon']/30).round(0)\n",
    "\n",
    "# Since we derived a new column from 'aon', we can drop it\n",
    "churn_filtered.drop('aon',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of he tenure variable\n",
    "\n",
    "sns.distplot(churn_filtered['tenure'],bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae23478",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_range = [0, 6, 12, 24, 60, 61]\n",
    "tn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "churn_filtered['tenure_range'] = pd.cut(churn_filtered['tenure'], tn_range, labels=tn_label)\n",
    "churn_filtered['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot for tenure range\n",
    "plt.figure(figsize=[12,7])\n",
    "sns.barplot(x='tenure_range',y='churn', data=churn_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7718a19",
   "metadata": {},
   "source": [
    "It can be seen that the maximum churn rate happens within 0-6 month, but it gradually decreases as the customer retains in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca8fcb",
   "metadata": {},
   "source": [
    "The average revenue per user is good phase of customer is given by arpu_6 and arpu_7. since we have two seperate averages, lets take an average to these two and drop the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered[\"avg_arpu_6_7\"]= (churn_filtered['arpu_6']+churn_filtered['arpu_7'])/2\n",
    "churn_filtered['avg_arpu_6_7'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop the original columns as they are derived to a new column for better understanding of the data\n",
    "\n",
    "churn_filtered.drop(['arpu_6','arpu_7'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# The curent dimension of the dataset after dropping few unwanted columns\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the column created\n",
    "sns.distplot(churn_filtered['avg_arpu_6_7'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Correlation between target variable(SalePrice) with the other variable in the dataset\n",
    "plt.figure(figsize=(10,50))\n",
    "heatmap_churn = sns.heatmap(churn_filtered.corr()[['churn']].sort_values(ascending=False, by='churn'),annot=True, \n",
    "                                cmap='summer')\n",
    "heatmap_churn.set_title(\"Features Correlating with Churn variable\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e55669",
   "metadata": {},
   "source": [
    ". Avg Outgoing Calls & calls on romaning for 6 & 7th months are positively correlated with churn.\n",
    ". Avg Revenue, No. Of Recharge for 8th month has negative correlation with churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now draw a scatter plot between total recharge and avg revenue for the 8th month\n",
    "churn_filtered[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n",
    "                                                              y='arpu_8')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating categories for month 8 column totalrecharge and their count\n",
    "churn_filtered['total_rech_data_group_8']=pd.cut(churn_filtered['total_rech_data_8'],[-1,0,10,25,100],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])\n",
    "churn_filtered['total_rech_num_group_8']=pd.cut(churn_filtered['total_rech_num_8'],[-1,0,10,25,1000],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=churn_filtered,x=\"total_rech_data_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_data_8 variable\\n\",churn_filtered['total_rech_data_group_8'].value_counts())\n",
    "plt.show()\n",
    "plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=churn_filtered,x=\"total_rech_num_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_num_8 variable\\n\",churn_filtered['total_rech_num_group_8'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad7fb7",
   "metadata": {},
   "source": [
    "As the number of recharge rate increases, the churn rate decreases clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c99660",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.drop(['av_rech_amt_data_8','total_rech_data_8','sachet_2g_6','sachet_2g_7','sachet_3g_6',\n",
    "              'sachet_3g_7','sachet_3g_8','last_day_rch_amt_6','last_day_rch_amt_7',\n",
    "              'last_day_rch_amt_8',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552cb2e",
   "metadata": {},
   "source": [
    "now we can drop all \"mou related\" columns  except onnet and offnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.drop(['loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou','roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8', \n",
    "         'roam_og_mou_6', 'roam_og_mou_7', 'roam_og_mou_8', 'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8',\n",
    "         'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8', 'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n",
    "         'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8', 'loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8', \n",
    "         'std_og_t2m_mou_6', 'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8', 'std_og_t2c_mou_6', 'std_og_t2c_mou_7',\n",
    "         'std_og_t2c_mou_8', 'std_og_mou_6', 'std_og_mou_7', 'std_og_mou_8', 'isd_og_mou_6', 'isd_og_mou_7', 'spl_og_mou_6',\n",
    "         'spl_og_mou_7', 'spl_og_mou_8','total_og_mou_6', 'loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8', \n",
    "         'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8', 'loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8',\n",
    "         'loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8', 'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8', \n",
    "         'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8', 'std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8',\n",
    "         'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8', 'std_ic_mou_6', 'spl_ic_mou_6', 'spl_ic_mou_7',\n",
    "         'spl_ic_mou_8', 'isd_ic_mou_6', 'isd_ic_mou_7', 'isd_ic_mou_8',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (50, 50))\n",
    "sns.heatmap(churn_filtered.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd28809",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.drop(['total_rech_data_group_8','total_rech_num_group_8',] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b31fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f17d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.drop(['tenure_range'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8529b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate = (sum(churn_filtered[\"churn\"])/len(churn_filtered[\"churn\"].index))*100\n",
    "churn_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04c471",
   "metadata": {},
   "source": [
    "# v.Split Data Into Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59da9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a67289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into train and test\n",
    "X = churn_filtered.drop(\"churn\", axis = 1)\n",
    "y = churn_filtered.churn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583de1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of train and test sets\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cf277",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54736e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f328b2",
   "metadata": {},
   "source": [
    "# vi.Performe Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c69cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = X_train.select_dtypes(include = ['int64','float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bad634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling on the dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[num_col] = scaler.fit_transform(X_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ca7c7",
   "metadata": {},
   "source": [
    "As there are many variables we will start the process of dropping variables after doing the RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0ec60",
   "metadata": {},
   "source": [
    "# Data Modeling and Model Evaluation and Prepare Model for Predictor variables selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06225d95",
   "metadata": {},
   "source": [
    "## Data Imbalance Handling\n",
    "Using SMOTE method, we can balance the data w.r.t. churn variable and proceed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9997f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d030b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of X_train_sm Shape:\", X_train_sm.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4403b9d",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b822b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for Model creation\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train_sm,(sm.add_constant(X_train_sm)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40771551",
   "metadata": {},
   "source": [
    "# Logistic Regression using Feature Selection (RFE method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec506cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# running RFE with 20 variables as output\n",
    "rfe = RFE(logreg,  n_features_to_select= 20)             \n",
    "rfe = rfe.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fff91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns=X_train_sm.columns[rfe.support_]\n",
    "print(\"The selected columns by RFE for modelling are: \\n\\n\",rfe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_sm.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b060457",
   "metadata": {},
   "source": [
    "# Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f74d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6174db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_sm_pred = res.predict(X_train_SM)\n",
    "y_train_sm_pred = y_train_sm_pred.values.reshape(-1)\n",
    "y_train_sm_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc275d7",
   "metadata": {},
   "source": [
    "# Creating a dataframe with the actual churn flag and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final = pd.DataFrame({'Converted':y_train_sm.values, 'Converted_prob':y_train_sm_pred})\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2812c",
   "metadata": {},
   "source": [
    "# Creating new column 'churn_pred' with 1 if Churn_Prob > 0.8 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19503f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final['churn_pred'] = y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Viewing the prediction results\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58639bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Confusion matrix \n",
    "confusion = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ced31",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "# Predicted     not_churn    churn\n",
    "# Actual\n",
    "\n",
    "# not_churn     15846           4823\n",
    "                    \n",
    "# churn             3950            16719  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy.\n",
    "print(\"The overall accuracy of the model is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2b1a",
   "metadata": {},
   "source": [
    "# Check for the VIF values of the feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072036f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73daa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_sm[rfe_columns].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_sm[rfe_columns].values, i) for i in range(X_train_sm[rfe_columns].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56e7d8",
   "metadata": {},
   "source": [
    "## Metrics beyond simply accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a234cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bc9c1",
   "metadata": {},
   "source": [
    "#### Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot the roc curve\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Prediction Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001df01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18e396",
   "metadata": {},
   "source": [
    "#### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e36820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e1b39",
   "metadata": {},
   "source": [
    "**Initially we selected the optimm point of classification as 0.5.<br><br>From the above graph, we can see the optimum cutoff is slightly higher than 0.5 but lies lower than 0.6. So lets tweek a little more within this range.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4655d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with refined probability cutoffs \n",
    "numbers = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11093833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dfd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df8c2d",
   "metadata": {},
   "source": [
    "**From the above graph we can conclude, the optimal cutoff point in the probability to define the predicted churn variabe converges at `0.53`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4799d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### From the curve above,we can take 0.53 is the optimum point to take it as a cutoff probability.\n",
    "\n",
    "y_train_sm_pred_final['final_churn_pred'] = y_train_sm_pred_final.Converted_prob.map( lambda x: 1 if x > 0.53 else 0)\n",
    "\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e01005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ovearall accuracy again\n",
    "print(\"The overall accuracy of the model now is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred )\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1081fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP2 = confusion2[1,1] # true positive \n",
    "TN2 = confusion2[0,0] # true negatives\n",
    "FP2 = confusion2[0,1] # false positives\n",
    "FN2 = confusion2[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP2 / float(TP2+FN2))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN2 / float(TN2+FP2))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP2/ float(TN2+FP2))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP2 / float(TP2+FP2))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN2 / float(TN2 + FN2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b3a6e",
   "metadata": {},
   "source": [
    "#### Precision and recall tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c95e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)\n",
    "\n",
    "# Plotting the curve\n",
    "plt.plot(thresholds, p[:-1], \"g-\")\n",
    "plt.plot(thresholds, r[:-1], \"r-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683ba8f",
   "metadata": {},
   "source": [
    "### Making predictions on the test set\n",
    "**Transforming and feature selection for test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test data\n",
    "X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "X_test=X_test[rfe_columns]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to the test model.\n",
    "X_test_SM = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d16d5f",
   "metadata": {},
   "source": [
    "## Predicting the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c786c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(X_test_SM)\n",
    "print(\"\\n The first ten probability value of the prediction are:\\n\",y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16354422",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_test_pred)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce90299",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred.rename(columns = {0:\"Conv_prob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = pd.concat([y_test_df,y_pred],axis=1)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['test_churn_pred'] = y_pred_final.Conv_prob.map(lambda x: 1 if x>0.54 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a35f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy of the predicted set.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc83d3",
   "metadata": {},
   "source": [
    "**Metrics Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "confusion2_test = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.test_churn_pred)\n",
    "print(\"Confusion Matrix\\n\",confusion2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed699fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating model validation parameters\n",
    "TP3 = confusion2_test[1,1] # true positive \n",
    "TN3 = confusion2_test[0,0] # true negatives\n",
    "FP3 = confusion2_test[0,1] # false positives\n",
    "FN3 = confusion2_test[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP3 / float(TP3+FN3))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN3 / float(TN3+FP3))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP3/ float(TN3+FP3))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP3 / float(TP3+FP3))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN3 / float(TN3+FN3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7efe7",
   "metadata": {},
   "source": [
    "### Explaining the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db98777",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of the predicted model is: \",round(metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred),2)*100,\"%\")\n",
    "print(\"The sensitivity of the predicted model is: \",round(TP3 / float(TP3+FN3),2)*100,\"%\")\n",
    "\n",
    "print(\"\\nAs the model created is based on a sentivity model, i.e. the True positive rate is given more importance as the actual and prediction of churn by a customer\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for the test dataset\n",
    "\n",
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.churn,y_pred_final.Conv_prob, drop_intermediate = False )\n",
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_pred_final.churn,y_pred_final.Conv_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffd632",
   "metadata": {},
   "source": [
    "## The AUC score for train dataset is 0.86 and the test dataset is 0.85.\n",
    "# This model can be considered as a good model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155119c",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5265c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bb4c7",
   "metadata": {},
   "source": [
    "### Analysing the explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527929d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=15, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03788737",
   "metadata": {},
   "source": [
    "we can use IncrementalPCA for the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e66c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_final = IncrementalPCA(n_components=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f22b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca = pca_final.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b452805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb9077",
   "metadata": {},
   "source": [
    "## Applying logistic regression on the Principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d467500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b509f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_pca = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = learner_pca.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce362d0",
   "metadata": {},
   "source": [
    "## Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = model_pca.predict_proba(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55262184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36a97a",
   "metadata": {},
   "source": [
    "### Confusion matrix, Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test1 = model_pca.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_test, pred_probs_test1)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_test, pred_probs_test1))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3568c",
   "metadata": {},
   "source": [
    "## Making predictions on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_train = model_pca.predict_proba(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:2.2}\".format(metrics.roc_auc_score(y_train, pred_probs_train[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d5c85",
   "metadata": {},
   "source": [
    "### Confusion matrix, Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_train1 = model_pca.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train, pred_probs_train1)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520531bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:-\",metrics.accuracy_score(y_train, pred_probs_train1))\n",
    "\n",
    "# Sensitivity\n",
    "print(\"Sensitivity:-\",TP / float(TP+FN))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity:-\", TN / float(TN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f613fd7",
   "metadata": {},
   "source": [
    "## Decision Tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e279c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e482ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8792523",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [2, 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'min_samples_split': [50, 150, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=dt, \n",
    "                           param_grid=params, \n",
    "                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f086be",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(grid_search.cv_results_)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.nlargest(5,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best = DecisionTreeClassifier( random_state = 42,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_leaf=20,\n",
    "                                  min_samples_split=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e40062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dt_classifier):\n",
    "    print(\"Train Accuracy :\", accuracy_score(y_train, dt_classifier.predict(df_train_pca)))\n",
    "    print(\"Train Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_train, dt_classifier.predict(df_train_pca)))\n",
    "    print(\"-\"*50)\n",
    "    print(\"Test Accuracy :\", accuracy_score(y_test, dt_classifier.predict(df_test_pca)))\n",
    "    print(\"Test Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, dt_classifier.predict(df_test_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73722bbf",
   "metadata": {},
   "source": [
    "##  Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, max_depth=4, max_features=5, random_state=100, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ac77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(rf, df_train_pca, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0767c",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning for the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafe4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04700cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [2,3,5],\n",
    "    'min_samples_leaf': [50,100],\n",
    "    'min_samples_split': [ 100, 150, ],\n",
    "    'n_estimators': [100, 200 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff611b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=params,\n",
    "                           cv = 4,\n",
    "                           n_jobs=-1, verbose=1, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40298a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=5,\n",
    "                             min_samples_leaf=50, \n",
    "                             min_samples_split=100,\n",
    "                             n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40805558",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rfc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1466e2b",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "Note that the best parameters procuded the accuracy of 91% which is not significantly deterred than the accuracy of original random forest, which is pegged around 92%\n",
    "\n",
    "## Conclusion :\n",
    "\n",
    "The best model to predict the churn is observed to be Random Forest based on the accuracy as performance measure.\n",
    "\n",
    "\n",
    "The incoming calls (with local same operator mobile/other operator mobile/fixed lines, STD or Special) plays a vital role in understanding the possibility of churn. Hence, the operator should focus on incoming calls data and has to provide some kind of special offers to the customers whose incoming calls turning lower.\n",
    "\n",
    "## Details:\n",
    "\n",
    " After cleaning the data, we broadly employed three models as mentioned below including some variations within these models in order to arrive at the best model in each of the cases.\n",
    "\n",
    "### Logistic Regression  :\n",
    "\n",
    "Logistic Regression with RFE Logistic regression with PCA Random Forest For each of these models, the summary of performance measures are as follows:\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    ".  Train Accuracy : ~79%\n",
    ". Test Accuracy : ~80%\n",
    "\n",
    "#### Logistic regression with PCA\n",
    "\n",
    ". Train Accuracy : ~91%\n",
    ". Test Accuracy : ~92%\n",
    "\n",
    "#### Decision Tree with PCA:\n",
    "\n",
    ". Train Accuracy : ~93%\n",
    ". Test Accuracy : ~92%\n",
    "\n",
    "\n",
    "#### Random Forest with PCA:\n",
    ". Train Accuracy :~ 91%\n",
    ". Test Accuracy :~ 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef180d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58defb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05b36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84da94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83477d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42368b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230acadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e64e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
